import argparse
import logging
import json
import os
import sys

from bugswarm.common import log


def main(argv):
    log.config_logging(getattr(logging, 'INFO', None))
    in_paths, out_path = _validate_input(argv)

    buildpairs = []
    tasks = []
    for path in in_paths:
        with open(path) as f:
            # Get task names to check for previous caching output CSVs
            tasks.append(str(os.path.splitext(path)[0].split('/')[-1]))
            buildpairs += json.load(f)

    to_be_cached = []
    for bp in buildpairs:
        # Make sure language is Java or Python
        java_jobs = []
        python_jobs = []
        for job in bp['failed_build']['jobs']:
            if job['language'] == 'java':
                java_jobs.append(job['job_id'])
            if job['language'] == 'python':
                python_jobs.append(job['job_id'])
        for job in bp['passed_build']['jobs']:
            if job['language'] == 'java':
                java_jobs.append(job['job_id'])
            if job['language'] == 'python':
                python_jobs.append(job['job_id'])

        # Cache all reproducible & unfiltered job pairs that use Java & Maven
        prefix = bp['repo'].replace('/', '-') + '-'
        for jp in bp['jobpairs']:
            # Skip unreproducible job pairs (flaky pairs are fine)
            if not any(match == 1 for match in jp['match_history'].values()):
                continue

            should_be_cached = (not jp['is_filtered'] and
                                jp['build_system'] in {'Maven', 'Gradle'} and
                                jp['failed_job']['job_id'] in java_jobs and
                                jp['passed_job']['job_id'] in java_jobs)
            if should_be_cached:
                to_be_cached.append(prefix + str(jp['failed_job']['job_id']))
                continue

            should_be_cached = (not jp['is_filtered'] and
                                jp['failed_job']['job_id'] in python_jobs and
                                jp['passed_job']['job_id'] in python_jobs)
            if should_be_cached:
                to_be_cached.append(prefix + str(jp['failed_job']['job_id']))
                continue

    try:
        os.mkdir('input')
    except FileExistsError:
        pass

    cached_image_tags = set()
    for task in tasks:
        if os.path.isfile('../github-cacher/output/{}'.format(task)):
            with open('../github-cacher/output/{}.csv'.format(task)) as f:
                for row in f:
                    row_list = row.split(', ')
                    if row_list[1] == 'succeed':
                        cached_image_tags.add(row_list[0])

    with open(out_path, 'w') as f:
        for image_tag in to_be_cached:
            if image_tag not in cached_image_tags:
                f.write(image_tag + '\n')
    log.info('Wrote file to {}/{}'.format(os.getcwd(), out_path))


def _validate_input(argv):
    parser = argparse.ArgumentParser()
    parser.add_argument('-i', '--input-path', required=True, nargs='+',
                        help='Path to an input file generated by the Reproducer. Multiple files can be specified.')
    parser.add_argument('-o', '--task-name', required=True,
                        help='Name of task.')
    opts = parser.parse_args(argv[1:])

    for path in opts.input_path:
        if not os.path.isfile(path):
            print('"{}" does not exist or is not a regular file. Exiting.'.format(path))
            parser.print_usage()
            sys.exit(1)

    return opts.input_path, 'input/' + opts.task_name


if __name__ == '__main__':
    sys.exit(main(sys.argv))
